Introduction and Decision Trees
===============================

**Unsupervised Learning**: The agent learns patterns in the input even though no explicit feedback is supplied. Most common unsupervised learning-task is clustering: detecting potentially useful clusters of input examples. For example, a taxi agent might gradually develop a concept of "good traffic days" and "bad traffic days" without ever being given labeled examples.
**Reinforcement learning**: The agent learns from a series of reinforcements - rewards or punishments. 
**Supervised learning**: The agent observes some example input - output pairs and learns a function that maps from input to output. The outputs can come from a teacher who gives the agent information about what the output is. The output can also come from the agent's percepts and the environment ends up being the teacher.

Noise and lack of labels create a continuum between supervised and unsupervised learning. 

Supervised Learning
-------------------

The task of supervised learning is this:

Given a **training set** of *N* example input-output pairs `(x1, y1), (x2, y2),  ..., (xN, yN)`, where each `yj` was generated by an unkknown funciton `y = f(x)`, discover a function `h` that approximates the true function `f`. 

`x` and `y` can be any value; they need not be numbers. The function `h` is a **hypothesis**. Learning is a searhch through the space of possible hypotheses for one that will perform well, even on new examples beyond the training set. To measure the accuracy of a hhypothesis we give it a **test set** of examples that are distinct from the training set. We say a hypothesis **generalizes** well if it correctly predicts the value of *y* for novel examples. Sometimes the function ``f` is **stochastic** - it is not strictly a function of `x`, and what we have to learn is a **conditional probability** distribution `P(Y | x)`.

**Classification**: When the output `y` is one of a finit set of values (such as `sunny`, `cloudy`, or `rainy`), the learning problem is called **classification**, and is called boolean or binary classification if there are only two values.

**Regression**: When `y` is a number (such as tomorrow's temperature), the learning problem is called **regression**. (Technically, solving a regression problem is finding a conditional expectation or average value of `y`, because the probability that we have found *exactly* the right real-valued number for `y` is 0.)

How do we choose from among multiple, consistent hypotheses? One answer is to prefer the *simplest* hypothesis consistent with the data. This principle is called **Ockham's razor**.  In general there is a tradeoff between complex hypotheses that fit the training data well, and simpler hypotheses that may generalize better (i.e., the question of overfitting). 

We say a learning problem is **realizable** if the hypothesis space contains the true function. Unfortunately we cannot always tell whether a given learning problem is realizable. 

Learning Decision Trees
-----------------------

A **decision tree** represents a function that takes as input a vector of attribute values and returns a "decision" - a single output value. The input and output values can be discrete or continuous. A decision tree reaches its decision by performing a sequence of tests. Each internal node in the tree corresponds to a test of the value of one of the input attributes, `Ai`, and the branches from the node are labled with the possible values of the attribute, `A_i = v_ik`. Each leaf node in the tree specifies a value to be returned by the function. 

Some functions cannot be represented concisely. For example, the majority function, which returns true if and only if omre than half of the inputs are true, requires an exponentially large decision tree. Decision trees are therefore good for some kinds of functions and bad for others. There no one representation that is efficient for all kinds of funtions. For example, consider the set of all boolean functions on n attributes. How many different functions are in this set? This is the number of different truth tables we can write down. A truth table over n attributes has `2^n` rows, one for each combination of values of the attributes. We can consider the answer column of the table as a `2^n`-bit number that defines the function. THerefore there are `2^(2^n)` different functions. 

Finding a minimal decision tree consistent with the training set is NP-hard. Constructing a minimal binary tree with respect to the expected number of tests required for classifying an unseen instance is NP-complete. Even finding the minimal equivalent decision tree for a given decision tree, or building the optimal decision tree from decision tables is known to be NP-hard. (pg 699)

**Inducing decision trees from examples**

An example for a decision tree consists of an `(x, y)` pair where **`x`** is a verctor of values for the input attributes, and `y` is a single Boolean output value. 

It is guided by four cases:

 1. If the remaining examples are all positive (or all neative), then we are done: we can answer *Yes* or *No*. (e.g., see None and Some branches on pg 701). 
 2. If there are some positive and negative examples, then choose the best attribute to split them. (see Hungry being used to split on pg 701).
 3. If there are no examples left, it means that no example has been observed for this combination of attribute values, and we return a default value calculated from the plurality classification of all examples that we used in contructing the node's parent. (passed along in `parent_examples`).
 4. If there are no attributes left, but both positive and negative examples, it means that these examples have exactly the same description, but different classifications. This can happen because there is an error or noise in the data. 

The set of examples is crucial for *constructing* the tree, but do not appear anywhere in the tree itself. 

We can **evaluate** the accuracy of a learning algorithms with a **learning curve**. We split the examples into a training set and test set. We learn a hypothesis *h* with the training set and measure its accuracy with the test set. For example, if we have 100 examples, we start with a training set of size 1 and increase one at a time up to size 99. For each size, we repeat the process of randomly splitting 20 times, and average the results of the 20 trials. 

**Choosing attribute tests**

We need a formal measure of "fairly good" and "really useless". We can do this with the notion of information gain, which is defined in terms of **entropy**. (Entropy formula pg 704. Entropy is the sum of the probability of each value vk of the random variable V multipled by the inverse of the log (base 2) probability of that variable (vk). 

Hence for a boolean variable, it is just `B(q) = -(qlog2q + (1 - q)log2(1-q))`. Since a boolean variable can only have two values. The probability of one value is always 1 minus probability of the other. So in the case of the decision tree, we can look at the entropy of the goal attribute on the whole set. We can simply look at one example, the positive example, since the formula accounts for the negative example as well `(1 - q)`. So we can simply calculate `H(Goal)`, which is just `B(p / (p + n))` (i.e., we see the percentage of how many times the positive example happens in the example set). 

Now testing on a single attribute will only give us part of the bits of `B(p / (p + n))`. In the restaurant example, testing one attribute only gives us part of the information of 1 bit (of entropy; p = n = 6 so B = 1). We can measure exactly how much by looking at the entropy remaining *after* the attribute test. 

**Generalization and overfitting**

An algorithm may accurately predict every case according to the test data, but it may not be general. This is called overfitting. For decision trees, we can use **decision tree pruning** to combat overfitting. Pruning works by eliminating nodes in the decision tree that are not clearly relevant. We start with a full tree, and then look at a test node that has only leaf nodes as descendants. If the test appears to be irrelevant (detecting only noise in the data), we eliminate the test, replacing it with a leaf node. This process is repeated, considering each test with only leaf descendants, until each one has either been pruned or accepted as is. 

How do we detect irrelevance? Suppose we are at a node consisting of *p* positive and *n* negative examples. If the attribute is irrelevant, we would expect that it would split the examples into subsets that each have roughly the same proprtion of positive examples as the whole set, `p / (p + n)`, and so the information gain will be close to zero. Hence information gain is a good clue to irrelevance. How large a gain should we require in order to split on a particular attribute?

To answer that we do a statistical **significance test**. Such a test begins by assuming there is no underlying pattern (the so-called **null hypothesis**). The the actual data are analyzed to calculate to the extent to which they deviate from a perfect absence of pattern. If the degree of deviation is statistically unlikely (usually taken to mean a 5% probability or less), then that is considered to be good evidence for the presence of a significant pattern in the data. This is basically chi-squared.

So we need observed and expected values for each subset (i.e., for every Ek from 1 to d). The expected values would then just be `p * ((pk + nk) / (p + n))` (for positive) and `n * ((pk + nk) / (p + n))` (for negative). The total deviation would be the overall sum for each k of the sum of the positive and negative chi-squared values. (pg 706). This value delta is distributed according to chi-squared with `v - 1` degress of freedom (where `v` is the number of values for the attribute). 

**Broadening the applicability of decision trees**

See pg 707

 - **Missing data** Given a complete decision tree, how should one classify an example that is missing one of the test attributes? Second, how should one modify the information-gain formula, when some examples have unknown values for the attribute? 
 - **Multivalued attributes** How do you split on something like *ExactTime* that has a bunch of different values that are effectively singletons?
 - **Continuous and integer-valued input attributes** Something like *Height* and *Weight* would have an infinite set of possible values. Decision trees usually try to find a **split point** like, *Weight* > 160. 
 - **Continuous-valued output attributes** If we are trying to predict a numerical output value, we need a **regression tree** rather than a classification tree. A regression tree has at each leaf a linear function of some subset of numerical attributes rather than a single value. Learning algorithm must decide when to stop splitting and begin applying linear regression over the attributes.

Evaluating and Choosing the Best Hypothesis
-------------------------------------------

We want to learn a hypothesis that fits the future data best. We make the **stationary assumption**: that there is a probability distribution over examples that remains stationary over time (pg 708). Examples that satisfy these assumptions are called *independent and identically distributed* or **i.i.d.**. This is a definition for our "future data". 

What is "best fit"? First we define the **error rate** of a hypothesis as the proportion of mistakes it makes. That is the number of times `h(x) != y` for an `(x, y)` example. It does not mean, however, that because the hypothesis has a low error rate on the training set, it will generalize well. To get an accurate evaluation of a hypothesis, we need to test it on a set of examples it has not seen yet.

The simplest approach is **holdout cross-validation**. Randomly split the available data into a training set from which the learning algorithm produces `h` and a test set on which the accuracy of `h` is evaluated. This has the disadvantage that it fails to use all of the available data (pg 708). 

Another technique is *k*-**fold cross-validation**. Each example serves double duty - as training data and test data. First we split the data into *k* equal subsets. We then perform *k* rounds of learning; on each round, *1/k* of the data is held out as a test set and the remaining examples are used as training data. The average test set score of the *k* rounds should then be a better extimate than a single score. Popular values for *k* are 5 and 10. This takes us 5-10 times longer to run but is statistically more-accurate. The extreme is *k = n* or **leave-one-out cross-validation** or **LOOCV**.

**Peeking** causes the test results to be invalidated. Users can inadvertently **peek** at the test data. If you select the hypothesis *on the basis of its test set error rate*, you have peeked.

The best way is to lock your test set away and only use it to validate. 

**Model selection: Complexity versus goodness of fit**

(pg 709, 710)

Higher-degree polynomials can fit the training data better, but when the degree is too high, they will overfit and perform poorly on validation data. Choosing the degree of the polynomial is an instance of the problem of **model selection**. You can think of the task of finding the best hypothesis as two tasks: model selection defines the hypothesis space and then **optimization** finds the best hypothesis within that space.

This section talks about selecting models that are parameterizing by *size*.


